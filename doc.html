<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AR Face Recognition Video Player - Technical Documentation</title>
    <style>
        /* PDF-like styling */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        }
        
        body {
            background-color: #f5f5f5;
            color: #333;
            line-height: 1.6;
            padding: 20px;
        }
        
        .pdf-container {
            max-width: 210mm;
            min-height: 297mm;
            margin: 0 auto;
            background: white;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            padding: 25mm;
            position: relative;
        }
        
        .page-break {
            page-break-after: always;
        }
        
        h1 {
            font-size: 28px;
            margin-bottom: 20px;
            color: #2c3e50;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
        }
        
        h2 {
            font-size: 22px;
            margin: 25px 0 15px;
            color: #2980b9;
            padding-bottom: 5px;
            border-bottom: 1px solid #eee;
        }
        
        h3 {
            font-size: 18px;
            margin: 20px 0 10px;
            color: #34495e;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul, ol {
            margin-left: 30px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        code {
            background-color: #f8f8f8;
            border: 1px solid #e1e1e8;
            border-radius: 3px;
            padding: 2px 6px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }
        
        pre {
            background-color: #f8f8f8;
            border: 1px solid #e1e1e8;
            border-radius: 5px;
            padding: 15px;
            margin: 15px 0;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
            line-height: 1.4;
        }
        
        .code-block {
            background-color: #2d2d2d;
            color: #f8f8f2;
            padding: 15px;
            border-radius: 5px;
            margin: 15px 0;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
            line-height: 1.4;
        }
        
        .note {
            background-color: #e7f3ff;
            border-left: 4px solid #3498db;
            padding: 15px;
            margin: 15px 0;
        }
        
        .warning {
            background-color: #fff8e1;
            border-left: 4px solid #ffc107;
            padding: 15px;
            margin: 15px 0;
        }
        
        .success {
            background-color: #e8f5e9;
            border-left: 4px solid #4caf50;
            padding: 15px;
            margin: 15px 0;
        }
        
        .header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 2px solid #3498db;
        }
        
        .header h1 {
            border: none;
            margin-bottom: 10px;
        }
        
        .header .subtitle {
            font-size: 18px;
            color: #7f8c8d;
        }
        
        .footer {
            position: absolute;
            bottom: 20px;
            width: calc(100% - 50mm);
            text-align: center;
            font-size: 12px;
            color: #7f8c8d;
            border-top: 1px solid #eee;
            padding-top: 10px;
        }
        
        .table-of-contents {
            margin: 20px 0;
            padding: 15px;
            background-color: #f8f9fa;
            border-radius: 5px;
        }
        
        .toc-item {
            margin-bottom: 8px;
        }
        
        .toc-item a {
            text-decoration: none;
            color: #3498db;
        }
        
        .toc-item a:hover {
            text-decoration: underline;
        }
        
        .feature-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        
        .feature-card {
            background: #f8f9fa;
            border-radius: 5px;
            padding: 15px;
            border-left: 4px solid #3498db;
        }
        
        .feature-card h4 {
            margin-bottom: 10px;
            color: #2c3e50;
        }
        
        .architecture-diagram {
            text-align: center;
            margin: 20px 0;
        }
        
        .architecture-diagram img {
            max-width: 100%;
            border: 1px solid #ddd;
            border-radius: 5px;
        }
        
        .diagram-placeholder {
            background: #f8f9fa;
            border: 1px dashed #ccc;
            border-radius: 5px;
            padding: 40px;
            text-align: center;
            color: #7f8c8d;
            margin: 20px 0;
        }
        
        @media print {
            body {
                background: white;
                padding: 0;
            }
            
            .pdf-container {
                box-shadow: none;
                padding: 15mm;
                max-width: 100%;
                min-height: auto;
            }
            
            .page-break {
                page-break-after: always;
            }
            
            .no-print {
                display: none;
            }
        }
    </style>
</head>
<body>
    <div class="pdf-container">
        <div class="header">
            <h1>AR Face Recognition Video Player</h1>
            <div class="subtitle">Complete Technical Documentation</div>
            <p>Version 1.0 | Last Updated: October 2023</p>
        </div>
        
        <div class="table-of-contents">
            <h2>Table of Contents</h2>
            <div class="toc-item"><a href="#introduction">1. Introduction</a></div>
            <div class="toc-item"><a href="#overview">2. System Overview</a></div>
            <div class="toc-item"><a href="#features">3. Features</a></div>
            <div class="toc-item"><a href="#architecture">4. System Architecture</a></div>
            <div class="toc-item"><a href="#implementation">5. Implementation Details</a></div>
            <div class="toc-item"><a href="#setup">6. Setup & Installation</a></div>
            <div class="toc-item"><a href="#usage">7. Usage Guide</a></div>
            <div class="toc-item"><a href="#api">8. API Reference</a></div>
            <div class="toc-item"><a href="#troubleshooting">9. Troubleshooting</a></div>
            <div class="toc-item"><a href="#future">10. Future Enhancements</a></div>
        </div>
        
        <div class="page-break"></div>
        
        <h2 id="introduction">1. Introduction</h2>
        
        <p>The AR Face Recognition Video Player is a web-based application that combines real-time face detection and recognition with personalized video playback. Using advanced machine learning models from TensorFlow.js and face-api.js, the system can identify individuals through their webcam and play associated videos when a recognized face is detected.</p>
        
        <p>This application demonstrates the practical implementation of computer vision technologies in a user-friendly interface, featuring multiple themes, video stabilization, and a comprehensive management system for training data.</p>
        
        <div class="note">
            <p><strong>Note:</strong> This application runs entirely in the browser without requiring server-side processing for face recognition, making it both privacy-friendly and highly accessible.</p>
        </div>
        
        <h2 id="overview">2. System Overview</h2>
        
        <p>The AR Face Recognition Video Player operates through the following core workflow:</p>
        
        <ol>
            <li><strong>Face Detection:</strong> The system continuously analyzes the webcam feed to detect human faces using the TinyFaceDetector from face-api.js.</li>
            <li><strong>Face Recognition:</strong> Detected faces are compared against a database of known individuals using facial descriptors.</li>
            <li><strong>Video Association:</strong> When a recognized face is identified, the system plays a pre-associated video in an overlay window.</li>
            <li><strong>Video Stabilization:</strong> The video overlay includes stabilization features that track the user's face movement for an enhanced viewing experience.</li>
        </ol>
        
        <div class="architecture-diagram">
            <div class="diagram-placeholder">
                [System Architecture Diagram Would Appear Here]
                <p>Camera Input → Face Detection → Face Recognition → Video Playback</p>
            </div>
        </div>
        
        <h2 id="features">3. Features</h2>
        
        <div class="feature-grid">
            <div class="feature-card">
                <h4>Real-time Face Detection</h4>
                <p>Uses TensorFlow.js and face-api.js for high-performance face detection directly in the browser.</p>
            </div>
            
            <div class="feature-card">
                <h4>Face Recognition</h4>
                <p>Recognizes individuals from a trained database with configurable matching thresholds.</p>
            </div>
            
            <div class="feature-card">
                <h4>Personalized Video Playback</h4>
                <p>Plays specific videos when recognized faces are detected, with smooth transitions.</p>
            </div>
            
            <div class="feature-card">
                <h4>Video Stabilization</h4>
                <p>Advanced stabilization algorithm that follows face movement for improved viewing.</p>
            </div>
            
            <div class="feature-card">
                <h4>Multiple Themes</h4>
                <p>Four distinct color themes (Dark, Light, Purple, Green) with persistent user preferences.</p>
            </div>
            
            <div class="feature-card">
                <h4>Training Interface</h4>
                <p>Intuitive interface for adding new people with face images and associated videos.</p>
            </div>
            
            <div class="feature-card">
                <h4>Demo Mode</h4>
                <p>Built-in demonstration mode for testing without a camera or training data.</p>
            </div>
            
            <div class="feature-card">
                <h4>Mobile Support</h4>
                <p>Responsive design with specific instructions for mobile device setup.</p>
            </div>
        </div>
        
        <div class="page-break"></div>
        
        <h2 id="architecture">4. System Architecture</h2>
        
        <h3>4.1 Technical Stack</h3>
        
        <p>The application is built using the following technologies:</p>
        
        <ul>
            <li><strong>Frontend:</strong> HTML5, CSS3, JavaScript (ES6+)</li>
            <li><strong>Machine Learning:</strong> TensorFlow.js, face-api.js</li>
            <li><strong>Styling:</strong> CSS Grid, Flexbox, CSS Custom Properties (Variables)</li>
            <li><strong>Icons:</strong> Font Awesome 6.4.0</li>
            <li><strong>Storage:</strong> Browser LocalStorage for persistence</li>
        </ul>
        
        <h3>4.2 Core Components</h3>
        
        <p>The application consists of several key components:</p>
        
        <ul>
            <li><strong>Video Processing Pipeline:</strong> Handles camera input, face detection, and recognition</li>
            <li><strong>Face Database Manager:</strong> Manages training data and facial descriptors</li>
            <li><strong>Video Stabilizer:</strong> Implements smooth tracking of recognized faces</li>
            <li><strong>UI Controller:</strong> Manages theme switching, user interactions, and state management</li>
            <li><strong>Alert System:</strong> Provides user feedback through toast-style notifications</li>
        </ul>
        
        <h3>4.3 Data Flow</h3>
        
        <p>The application follows this data flow pattern:</p>
        
        <pre>
Camera Feed → Face Detection → Descriptor Extraction → Face Matching → Video Playback
     ↓              ↓                 ↓                 ↓             ↓
Webcam API    TinyFaceDetector   FaceRecognitionNet   FaceMatcher   HTML5 Video
        </pre>
        
        <h2 id="implementation">5. Implementation Details</h2>
        
        <h3>5.1 Face Detection & Recognition</h3>
        
        <p>The system uses three pre-trained models from face-api.js:</p>
        
        <ul>
            <li><code>tinyFaceDetector</code> - For efficient face detection</li>
            <li><code>faceLandmark68Net</code> - For facial landmark detection</li>
            <li><code>faceRecognitionNet</code> - For generating facial descriptors</li>
        </ul>
        
        <p>These models are loaded from the <code>/models</code> directory and initialize the recognition system.</p>
        
        <div class="code-block">
// Model loading implementation
async function loadFaceAPIModels() {
  await faceapi.nets.tinyFaceDetector.loadFromUri('models');
  await faceapi.nets.faceLandmark68Net.loadFromUri('models');
  await faceapi.nets.faceRecognitionNet.loadFromUri('models');
  // Models ready for face detection and recognition
}
        </div>
        
        <h3>5.2 Video Stabilization Algorithm</h3>
        
        <p>The VideoStabilizer class implements a smoothing algorithm that:</p>
        
        <ul>
            <li>Tracks face position over time using a buffer of recent positions</li>
            <li>Applies exponential smoothing to reduce jitter</li>
            <li>Adjusts video position and scale based on face movement</li>
            <li>Provides configurable smoothness and strength parameters</li>
        </ul>
        
        <div class="code-block">
class VideoStabilizer {
  constructor() {
    this.positionBuffer = [];
    this.maxBufferSize = 10;
    this.currentPosition = { x: 0, y: 0, scale: 1 };
    this.targetPosition = { x: 0, y: 0, scale: 1 };
  }
  
  updateTargetPosition(facePosition, faceSize) {
    // Calculate target position based on face location
    // Apply smoothing using position buffer
    // Update video overlay transform
  }
}
        </div>
        
        <h3>5.3 Theme System</h3>
        
        <p>The application uses CSS Custom Properties (variables) to implement theming:</p>
        
        <div class="code-block">
:root {
  --primary: #3b82f6;
  --bg-primary: #0f172a;
  --text-primary: #f1f5f9;
  /* ... more variables */
}

[data-theme="light"] {
  --primary: #3b82f6;
  --bg-primary: #ffffff;
  --text-primary: #1e293b;
  /* ... theme overrides */
}
        </div>
        
        <p>Themes are persisted in localStorage and applied dynamically through JavaScript.</p>
        
        <div class="page-break"></div>
        
        <h2 id="setup">6. Setup & Installation</h2>
        
        <h3>6.1 Prerequisites</h3>
        
        <ul>
            <li>Modern web browser with WebRTC support (Chrome, Firefox, Edge)</li>
            <li>Webcam for face detection functionality</li>
            <li>Internet connection for loading external libraries</li>
        </ul>
        
        <h3>6.2 Installation Steps</h3>
        
        <ol>
            <li>Download the application files including:
                <ul>
                    <li><code>index.html</code> - Main application file</li>
                    <li><code>models/</code> directory - Face detection models</li>
                    <li><code>face-api.min.js</code> - Face recognition library</li>
                </ul>
            </li>
            <li>Serve the files through a web server (required for camera access)</li>
            <li>Open the application in a supported browser</li>
            <li>Allow camera permissions when prompted</li>
        </ol>
        
        <div class="warning">
            <p><strong>Important:</strong> The application must be served over HTTP/HTTPS for camera access to work. Opening the HTML file directly from the file system will not allow camera functionality due to browser security restrictions.</p>
        </div>
        
        <h3>6.3 Model Files</h3>
        
        <p>The face-api.js models must be placed in a <code>models</code> directory relative to the HTML file. Required model files include:</p>
        
        <ul>
            <li><code>tiny_face_detector_model-weights_manifest.json</code></li>
            <li><code>tiny_face_detector_model-shard1</code></li>
            <li><code>face_landmark_68_model-weights_manifest.json</code></li>
            <li><code>face_landmark_68_model-shard1</code></li>
            <li><code>face_recognition_model-weights_manifest.json</code></li>
            <li><code>face_recognition_model-shard1</code></li>
            <li><code>face_recognition_model-shard2</code></li>
        </ul>
        
        <h2 id="usage">7. Usage Guide</h2>
        
        <h3>7.1 Adding People to the Recognition System</h3>
        
        <ol>
            <li>Enter the person's name in the "Person Name" field</li>
            <li>Select 1-5 clear face images of the person using the file upload</li>
            <li>Optionally add a video URL or upload a video file to associate with the person</li>
            <li>Click "Add Person" to train the recognition system</li>
        </ol>
        
        <div class="note">
            <p><strong>Tip:</strong> Use clear, well-lit face images with minimal obstructions for best recognition accuracy.</p>
        </div>
        
        <h3>7.2 Starting Face Recognition</h3>
        
        <ol>
            <li>Ensure the camera is positioned correctly with good lighting</li>
            <li>Click "Start Recognition" to begin the detection process</li>
            <li>The system will display detected faces with bounding boxes</li>
            <li>When a recognized face appears, the associated video will play in the overlay</li>
        </ol>
        
        <h3>7.3 Using Video Stabilization</h3>
        
        <p>The video stabilization feature can be enabled to make the video overlay follow face movement:</p>
        
        <ol>
            <li>Click "Enable Stabilizer" to activate the feature</li>
            <li>Adjust the smoothness slider to control how quickly the overlay responds to movement</li>
            <li>Adjust the strength slider to control how much the overlay moves with face position</li>
        </ol>
        
        <h3>7.4 Demo Mode</h3>
        
        <p>If you don't have a camera or training data, you can use the Demo Mode:</p>
        
        <ol>
            <li>Click "Try Demo Mode" to start the demonstration</li>
            <li>The system will simulate face detection and recognition</li>
            <li>You can still test the video playback and stabilization features</li>
        </ol>
        
        <div class="page-break"></div>
        
        <h2 id="api">8. API Reference</h2>
        
        <h3>8.1 Core Classes</h3>
        
        <h4>VideoStabilizer</h4>
        
        <p>Manages video overlay stabilization based on face position.</p>
        
        <table style="width:100%; border-collapse: collapse; margin: 20px 0;">
            <thead>
                <tr style="background-color: #f8f9fa;">
                    <th style="border: 1px solid #ddd; padding: 8px;">Method</th>
                    <th style="border: 1px solid #ddd; padding: 8px;">Description</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td style="border: 1px solid #ddd; padding: 8px;"><code>enable()</code></td>
                    <td style="border: 1px solid #ddd; padding: 8px;">Activates the stabilizer</td>
                </tr>
                <tr>
                    <td style="border: 1px solid #ddd; padding: 8px;"><code>disable()</code></td>
                    <td style="border: 1px solid #ddd; padding: 8px;">Deactivates the stabilizer</td>
                </tr>
                <tr>
                    <td style="border: 1px solid #ddd; padding: 8px;"><code>updateTargetPosition(facePosition, faceSize)</code></td>
                    <td style="border: 1px solid #ddd; padding: 8px;">Updates target position based on face detection</td>
                </tr>
                <tr>
                    <td style="border: 1px solid #ddd; padding: 8px;"><code>updateSettings(smoothness, strength)</code></td>
                    <td style="border: 1px solid #ddd; padding: 8px;">Updates stabilizer parameters</td>
                </tr>
            </tbody>
        </table>
        
        <h3>8.2 Key Functions</h3>
        
        <table style="width:100%; border-collapse: collapse; margin: 20px 0;">
            <thead>
                <tr style="background-color: #f8f9fa;">
                    <th style="border: 1px solid #ddd; padding: 8px;">Function</th>
                    <th style="border: 1px solid #ddd; padding: 8px;">Description</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td style="border: 1px solid #ddd; padding: 8px;"><code>showAlert(message, type, title, duration)</code></td>
                    <td style="border: 1px solid #ddd; padding: 8px;">Displays a notification to the user</td>
                </tr>
                <tr>
                    <td style="border: 1px solid #ddd; padding: 8px;"><code>handleAddPerson()</code></td>
                    <td style="border: 1px solid #ddd; padding: 8px;">Processes new person data and trains the model</td>
                </tr>
                <tr>
                    <td style="border: 1px solid #ddd; padding: 8px;"><code>mainLoop()</code></td>
                    <td style="border: 1px solid #ddd; padding: 8px;">Main recognition loop that processes video frames</td>
                </tr>
                <tr>
                    <td style="border: 1px solid #ddd; padding: 8px;"><code>setPersonVideo(src, personName)</code></td>
                    <td style="border: 1px solid #ddd; padding: 8px;">Plays the associated video for a recognized person</td>
                </tr>
            </tbody>
        </table>
        
        <h2 id="troubleshooting">9. Troubleshooting</h2>
        
        <h3>9.1 Common Issues</h3>
        
        <div class="warning">
            <p><strong>Camera not working:</strong> Ensure you're using HTTPS or localhost, and that you've granted camera permissions. Some browsers block camera access on insecure origins.</p>
        </div>
        
        <div class="warning">
            <p><strong>Models not loading:</strong> Check that all model files are in the correct location and that the server is properly configured to serve them.</p>
        </div>
        
        <div class="warning">
            <p><strong>Poor recognition accuracy:</strong> Use higher quality training images with consistent lighting and clear facial features. Consider adding more training images per person.</p>
        </div>
        
        <h3>9.2 Performance Optimization</h3>
        
        <ul>
            <li>Reduce the detection interval for better performance on slower devices</li>
            <li>Use fewer training images per person to reduce memory usage</li>
            <li>Close other browser tabs to free up system resources</li>
            <li>Use the TinyFaceDetector options to adjust the detection sensitivity</li>
        </ul>
        
        <div class="page-break"></div>
        
        <h2 id="future">10. Future Enhancements</h2>
        
        <p>The AR Face Recognition Video Player can be extended with several advanced features:</p>
        
        <h3>10.1 Planned Features</h3>
        
        <ul>
            <li><strong>Multi-face tracking:</strong> Simultaneous recognition and video playback for multiple people</li>
            <li><strong>Emotion detection:</strong> Play different videos based on detected emotions</li>
            <li><strong>Age and gender estimation:</strong> Additional demographic information for content personalization</li>
            <li><strong>Cloud synchronization:</strong> Sync recognition data across devices</li>
            <li><strong>Advanced analytics:</strong> Track recognition patterns and user engagement</li>
        </ul>
        
        <h3>10.2 Technical Improvements</h3>
        
        <ul>
            <li><strong>WebAssembly acceleration:</strong> Improve performance with WebAssembly versions of ML models</li>
            <li><strong>Progressive Web App:</strong> Add offline functionality and app-like experience</li>
            <li><strong>Server-side processing:</strong> Optional cloud-based recognition for improved accuracy</li>
            <li><strong>API endpoints:</strong> RESTful API for integration with other systems</li>
        </ul>
        
        <div class="success">
            <p><strong>Development Roadmap:</strong> The application is actively maintained with regular updates planned for performance improvements and new features.</p>
        </div>
        
        <div class="footer">
            <p>AR Face Recognition Video Player - Technical Documentation | Page 5 of 5</p>
        </div>
    </div>
    
    <div class="no-print" style="text-align: center; margin-top: 20px;">
        <button onclick="window.print()" style="padding: 10px 20px; background: #3498db; color: white; border: none; border-radius: 5px; cursor: pointer;">Print / Save as PDF</button>
    </div>
</body>
</html>